{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Investigating GPT-3 embeddings\n",
    "* Load data from text files\n",
    "* Assemble data structures\n",
    "* Define heplper functions\n",
    "* Investigate vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "import numpy as np\n",
    "from openai.embeddings_utils import get_embedding\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Set in conda\n",
    "openai.api_key = os.getenv('OPENAI_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for eng['ada']\n",
    "# space for other constants\n",
    "eng = {'ada': 'text-similarity-ada-001', 'bab': 'text-similarity-babbage-001'}\n",
    "\n",
    "vocab = []\n",
    "with open(u'/gpfs/fs1/home/mbarlow6/Desktop/Conceptual-Analysis/barlow/valid_vocab.txt', 'r') as f:\n",
    "    for w in f:\n",
    "        vocab.append(w.strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ada embeds: 1024 dims\n",
    "ada_embeds = []\n",
    "with open(u'/gpfs/fs1/home/mbarlow6/Desktop/Conceptual-Analysis/barlow/gpt/gpt_ada.txt', 'r') as f:\n",
    "    for line in f:\n",
    "        ada_embeds.append([float(x) for x in line.strip().split()])\n",
    "\n",
    "model_ada = dict(zip(vocab, ada_embeds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Babbage embeds: 2048 dims\n",
    "bab_embeds = []\n",
    "with open(u'/gpfs/fs1/home/mbarlow6/Desktop/Conceptual-Analysis/barlow/gpt/gpt_babbage.txt', 'r') as f:\n",
    "    for line in f:\n",
    "        bab_embeds.append([float(x) for x in line.strip().split()])\n",
    "\n",
    "model_bab = dict(zip(vocab, bab_embeds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper Functions\n",
    "* definition -> most_similar, positive\n",
    "* calculate_similarity ((a + b) -> t)\n",
    "* sum_complexity\n",
    "* above_zero_complexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def positive(words, model='ada'):\n",
    "    \"\"\"\n",
    "    Given a list of words, sum the embeddings. (matrix addition)\n",
    "    words: list[str]\n",
    "    model: str -> ['ada', 'bab']\n",
    "    \"\"\"\n",
    "    if model.lower() == 'ada':\n",
    "        mdl = model_ada\n",
    "    elif model.lower() == 'bab':\n",
    "        mdl = model_bab\n",
    "    else:\n",
    "        raise ValueError('Model selection must be \\'ada\\' or \\'bab\\'')\n",
    "\n",
    "    print(words)\n",
    "    res = np.array(mdl[words[0]]).reshape(1, -1)\n",
    "    for x in words[1:]:\n",
    "        res += np.array(mdl[x]).reshape(1, -1)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def most_similar(word, phrase=False, top_n=10, model='ada'):\n",
    "    \"\"\"\n",
    "    Return the 10 most similar words to a given word based on BERT embeddings\n",
    "    word: string\n",
    "    model: str -> ['ada', 'bab']\n",
    "    \"\"\"\n",
    "    if model.lower() == 'ada':\n",
    "        mdl = model_ada\n",
    "    elif model.lower() == 'bab':\n",
    "        mdl = model_bab\n",
    "    else:\n",
    "        raise ValueError('Model selection must be \\'ada\\' or \\'bab\\'')\n",
    "\n",
    "    similarities = {}\n",
    "    word1 = word\n",
    "    if not phrase:\n",
    "        word1 = np.array(mdl[word]).reshape(1, -1)\n",
    "    for term in vocab:\n",
    "        word2 = np.array(mdl[term]).reshape(1, -1)\n",
    "        if isinstance(word, str):\n",
    "            if term != word:\n",
    "                similarities[term] = cosine_similarity(word1, word2)\n",
    "        elif word2.any():\n",
    "            similarities[term] = cosine_similarity(word1, word2)\n",
    "    return list({k: v for k, v in sorted(similarities.items(), key=lambda item: item[1], reverse=True)}.items())[2:2+top_n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def definition(word1, word2, top_n=10, model='ada'):\n",
    "    \"\"\"\n",
    "    Return top_n most similar words to the positive of two word vectors.\n",
    "    word1, word2: str\n",
    "    top_n: int\n",
    "    model: str -> ['ada', 'bab']\n",
    "    \"\"\"\n",
    "    if model.lower() == 'ada':\n",
    "        mdl = model_ada\n",
    "    elif model.lower() == 'bab':\n",
    "        mdl = model_bab\n",
    "    else:\n",
    "        raise ValueError('Model selection must be \\'ada\\' or \\'bab\\'')\n",
    "    \n",
    "    return most_similar(positive([word1, word2]), True, top_n, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_similarity(words, target, model='ada'):\n",
    "    \"\"\"\n",
    "    Calculate the similarity between the positive of a list of words and a target\n",
    "    word1, word2, target: str\n",
    "    model: str -> ['ada', 'bab']\n",
    "    \"\"\"\n",
    "    if model.lower() == 'ada':\n",
    "        mdl = model_ada\n",
    "    elif model.lower() == 'bab':\n",
    "        mdl = model_bab\n",
    "    else:\n",
    "        raise ValueError('Model selection must be \\'ada\\' or \\'bab\\'')\n",
    "\n",
    "    # get the phrase vector\n",
    "    phrase = positive(words, model)\n",
    "\n",
    "    # get target vector\n",
    "    target = np.array(mdl[target]).reshape(1, -1)\n",
    "\n",
    "    return cosine_similarity(phrase, target)[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_connect(a, b, model='ada'):\n",
    "    \"\"\"\n",
    "    Given two word strings or feature vectors, calcuate their similarity.\n",
    "    a, b: str | list[float] (or ndarray)\n",
    "    \"\"\"\n",
    "    if model.lower() == 'ada':\n",
    "        mdl = model_ada\n",
    "    elif model.lower() == 'bab':\n",
    "        mdl = model_bab\n",
    "    else:\n",
    "        raise ValueError('Model selection must be \\'ada\\' or \\'bab\\'')\n",
    "\n",
    "    if isinstance(a, str):\n",
    "        a = np.array(mdl[a]).reshape(1, -1)\n",
    "    \n",
    "    if isinstance(b, str):\n",
    "        b = np.array(mdl[b]).reshape(1, -1)\n",
    "\n",
    "    return cosine_similarity(a, b)[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sum_complexity(word, model='ada'):\n",
    "    \"\"\"\n",
    "    Sums the absolute value of each dimension in a word's embedding vector\n",
    "    args:\n",
    "        word: str -> word to get embedding for\n",
    "    returns:\n",
    "        sum of absolute value of each dimension in embedding vector\n",
    "    \"\"\"\n",
    "    if model.lower() == 'ada':\n",
    "        mdl = model_ada\n",
    "    elif model.lower() == 'bab':\n",
    "        mdl = model_bab\n",
    "    else:\n",
    "        raise ValueError('Model selection must be \\'ada\\' or \\'bab\\'')\n",
    "\n",
    "    # should we keep absolute value here?\n",
    "    modified = [abs(float(x)) for x in mdl[word]]\n",
    "    return sum(modified)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def above_zero_complexity(word, threshold=0.05, model='ada'):\n",
    "    if model.lower() == 'ada':\n",
    "        mdl = model_ada\n",
    "    elif model.lower() == 'bab':\n",
    "        mdl = model_bab\n",
    "    else:\n",
    "        raise ValueError('Model selection must be \\'ada\\' or \\'bab\\'')\n",
    "    \n",
    "    emb = mdl[word]\n",
    "    modified = [abs(float(x)) for x in emb if abs(float(x)) > threshold]\n",
    "    return len(modified) / len(emb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vector investigation\n",
    "\n",
    "What's going on? Initial thoughts are leading me towards the angle of a vector (embedding) point us towards it's component ideas' \"meaning space\", and it's magnitude reveals something about the strength of connection to that meaning space. Higher magnitudes could mean that a vector is more \"embedded\" within a certain feature space, which lends validity to single word embeddings having a higher `sum_complexity` than paragraphs with many ideas.\n",
    "\n",
    "Cosine similarity only scores based on angle, not magnitude, which allows us to easily use positives to approximate combined meaning because we aren't worried about the magnitude of those positives. What if both are important? If complex ideas reside closer to 0 and this is because of their inherent complexity, then we must consider those values signifigant as well. The same argument generalizes to absolute value.\n",
    "\n",
    "If \"apple\" represents an idea space, then I would expect a short sentence all about apples to have a strong angle similarity to single word vectors related to apples (fruit, round, etc) and a story about apple picking to have strong angle similarity but small magnitude due to the influence of other major ideas in the story (context).\n",
    "\n",
    "I will begin to test such ideas below."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "1a58f2b50cba0e3ef60ddaac0060fcc6c1f1afd3fbbcd44f07b68475f7ee4549"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
