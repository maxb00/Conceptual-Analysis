{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning and Conceptual Complexity - Computational Methods for Music, Media, and Minds REU\n",
    "\n",
    "By Jens Kipper, Max Barlow, Sara Jo Jeiter-Johnson, and Tianyi Ma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "Our research focuses on determining if word embeddings in language models can show us information about conceptual complexity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing Language Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "# Get the interactive Tools for Matplotlib\n",
    "%matplotlib notebook\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from gensim.test.utils import datapath, get_tmpfile\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"For looking at word vectors, I'll use Gensim. We also use it in hw1 for word vectors. Gensim isn't really a deep learning package. It's a package for for word and text similarity modeling, which started with (LDA-style) topic models and grew into SVD and neural word representations. But its efficient and scalable, and quite widely used.\n",
    "\n",
    "Our homegrown Stanford offering is GloVe word vectors. Gensim doesn't give them first class support, but allows you to convert a file of GloVe vectors into word2vec format. You can download the GloVe vectors from [the Glove page](https://nlp.stanford.edu/projects/glove/). They're inside [this zip file](https://nlp.stanford.edu/data/glove.6B.zip)\n",
    "\n",
    "(I use the 100d vectors below as a mix between speed and smallness vs. quality. If you try out the 50d vectors, they basically work for similarity but clearly aren't as good for analogy problems. If you load the 300d vectors, they're even better than the 100d vectors.)\" -Tianyi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading word2vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "100d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_67309/201336454.py:3: DeprecationWarning: Call to deprecated `glove2word2vec` (KeyedVectors.load_word2vec_format(.., binary=False, no_header=True) loads GLoVE text vectors.).\n",
      "  glove2word2vec(glove_file, word2vec_glove_file)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(400000, 100)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glove_file = datapath(os.path.abspath(\"glove.6B/glove.6B.100d.txt\"))\n",
    "word2vec_glove_file = get_tmpfile(\"glove.6B.100d.word2vec.txt\")\n",
    "glove2word2vec(glove_file, word2vec_glove_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "300d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_67309/2309609736.py:3: DeprecationWarning: Call to deprecated `glove2word2vec` (KeyedVectors.load_word2vec_format(.., binary=False, no_header=True) loads GLoVE text vectors.).\n",
      "  glove2word2vec(glove_file_300d, word2vec_glove_file_300d)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(400000, 300)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glove_file_300d = datapath(os.path.abspath(\"glove.6B/glove.6B.300d.txt\"))\n",
    "word2vec_glove_file_300d = get_tmpfile(\"glove.6B.300d.word2vec.txt\")\n",
    "glove2word2vec(glove_file_300d, word2vec_glove_file_300d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = KeyedVectors.load_word2vec_format(word2vec_glove_file)\n",
    "model_300d = KeyedVectors.load_word2vec_format(word2vec_glove_file_300d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction\n",
    "\n",
    "Using pre-trained BERT langauge model to create word vectors for words and check their cosine similarity "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import torch\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BertModel.from_pretrained('bert-large-uncased',\n",
    "                                  output_hidden_states = True,\n",
    "                                  )\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-large-uncased')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(\"commonWords.txt\", \"r\")\n",
    "\n",
    "vocab += f.read().split(\"\\n\")\n",
    "\n",
    "vocab = list(np.unique(np.array(vocab)))[1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text prep and embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bert_text_preparation(text, tokenizer):\n",
    "    \"\"\"Preparing the input for BERT\n",
    "    \n",
    "    Takes a string argument and performs\n",
    "    pre-processing like adding special tokens,\n",
    "    tokenization, tokens to ids, and tokens to\n",
    "    segment ids. All tokens are mapped to seg-\n",
    "    ment id = 1.\n",
    "    \n",
    "    Args:\n",
    "        text (str): Text to be converted\n",
    "        tokenizer (obj): Tokenizer object\n",
    "            to convert text into BERT-re-\n",
    "            adable tokens and ids\n",
    "        \n",
    "    Returns:\n",
    "        list: List of BERT-readable tokens\n",
    "        obj: Torch tensor with token ids\n",
    "        obj: Torch tensor segment ids\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    marked_text = \"[CLS] \" + text + \" [SEP]\"\n",
    "    tokenized_text = tokenizer.tokenize(marked_text)\n",
    "    indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "    segments_ids = [1]*len(indexed_tokens)\n",
    "\n",
    "    # Convert inputs to PyTorch tensors\n",
    "    tokens_tensor = torch.tensor([indexed_tokens])\n",
    "    segments_tensors = torch.tensor([segments_ids])\n",
    "\n",
    "    return tokenized_text, tokens_tensor, segments_tensors\n",
    "    \n",
    "def get_bert_embeddings(tokens_tensor, segments_tensors, model):\n",
    "    \"\"\"Get embeddings from an embedding model\n",
    "    \n",
    "    Args:\n",
    "        tokens_tensor (obj): Torch tensor size [n_tokens]\n",
    "            with token ids for each token in text\n",
    "        segments_tensors (obj): Torch tensor size [n_tokens]\n",
    "            with segment ids for each token in text\n",
    "        model (obj): Embedding model to generate embeddings\n",
    "            from token and segment ids\n",
    "    \n",
    "    Returns:\n",
    "        list: List of list of floats of size\n",
    "            [n_tokens, n_embedding_dimensions]\n",
    "            containing embeddings for each token\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    # Gradient calculation id disabled\n",
    "    # Model is in inference mode\n",
    "    with torch.no_grad():\n",
    "        outputs = model(tokens_tensor, segments_tensors)\n",
    "        # Removing the first hidden state\n",
    "        # The first state is the input state\n",
    "        hidden_states = outputs[2][1:]\n",
    "\n",
    "    # Getting embeddings from the final BERT layer\n",
    "    token_embeddings = hidden_states[-1]\n",
    "    # Collapsing the tensor into 1-dimension\n",
    "    token_embeddings = torch.squeeze(token_embeddings, dim=0)\n",
    "    # Converting torchtensors to lists\n",
    "    list_token_embeddings = [token_embed.tolist() for token_embed in token_embeddings]\n",
    "\n",
    "    return list_token_embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get embeddings for vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting embeddings for the target word\n",
    "target_word_embeddings = []\n",
    "\n",
    "n = 0\n",
    "for text in vocab:\n",
    "    tokenized_text, tokens_tensor, segments_tensors = bert_text_preparation(text, tokenizer)\n",
    "    list_token_embeddings = get_bert_embeddings(tokens_tensor, segments_tensors, model)\n",
    "    \n",
    "    word_embedding = list_token_embeddings[1]\n",
    "    if text not in tokenized_text:\n",
    "        for idx in range(2,len(tokenized_text)-1):\n",
    "            word_embedding += list_token_embeddings[idx]\n",
    "        word_embedding = list(np.array(word_embedding)/len(tokenized_text))\n",
    "        word_embedding = np.array(word_embedding).reshape(-1, 1)\n",
    "        word_embedding = PCA().fit_transform(word_embedding)\n",
    "        word_embedding = word_embedding[:n,:]\n",
    "    elif n == 0:\n",
    "        n = len(word_embedding)\n",
    "\n",
    "    target_word_embeddings.append(word_embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing Similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gensim Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use 100d model and cosine_similarity to calculate similarity between a defining \n",
    "# phrase and the word.\n",
    "#\n",
    "# You can use doc2vec for this but there isn't any good pre-trained doc2vec models \n",
    "# and it takes too long to train my own model so I just averaged the two defining \n",
    "# words. This should make some sense.\n",
    "# \n",
    "# The Phrases model for word2vec doesn't support extracting individual word vectors\n",
    "\n",
    "def calculate_similarity(x1, x2, x3):\n",
    "    phrase = ((model[x1] + model[x2])/2).reshape(1, -1)\n",
    "    word = model[x3].reshape(1, -1)\n",
    "    result = cosine_similarity(phrase, word)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BERT functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_similarity_bert(x1, x2, x3):\n",
    "    phrase = (np.array(model_bert[x1]) + np.array(model_bert[x2])).reshape(1, -1)\n",
    "    word = np.array(model_bert[x3]).reshape(1, -1)\n",
    "    result = cosine_similarity(phrase, word)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing Complexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sum of the word vector\n",
    "\n",
    "def sum_complexity(x1):\n",
    "    models = ['BERT', 'GloVe_100d', 'GloVe_300d']\n",
    "    data = []\n",
    "    try:\n",
    "        data = [sum([abs(x) for x in model_bert[x1]]), sum(model[x1]), sum(model_300d[x1])]\n",
    "    except KeyError:\n",
    "        print(\"Key Error\")\n",
    "    return dict(zip(models, data))\n",
    "\n",
    "\n",
    "# count the number of features that has significant information\n",
    "# threshold = how far away a feature is from 0 (0 <= threshold <= 1). default = 0.5\n",
    "\n",
    "def above_zero_complexity(x1, threshold = 0.5):\n",
    "    models = ['BERT', 'GloVe_100d', 'GloVe_300d']\n",
    "    bert = len([x for x in model_bert[x1] if abs(x) >= threshold])/len(model_bert[x1])\n",
    "    glove_100d = 0\n",
    "    glove_300d = 0\n",
    "    try:\n",
    "        glove_100d = len([x for x in model[x1] if x >= threshold])/len(model[x1])\n",
    "        glove_300d = len([x for x in model_300d[x1] if x >= threshold])/len(model_300d[x1])\n",
    "    except KeyError:\n",
    "        print(\"Key Error\")\n",
    "    data = [bert, glove_100d, glove_300d]\n",
    "    return dict(zip(models, data))\n",
    "\n",
    "def calculate_complexity(words):\n",
    "    for word in words:\n",
    "        print(word)\n",
    "        print(\"Sum Complexity: \\n\", sum_complexity(word))\n",
    "        print(\"Above Zero Complexity: (threshold = 0.5)\\n\", above_zero_complexity(word))\n",
    "        print(\"Above Zero Complexity: (threshold = 0.3)\\n\", above_zero_complexity(word, 0.3))\n",
    "        print()\n",
    "\n",
    "def compare_complexity(x, y):\n",
    "    c1 = above_zero_complexity(x)\n",
    "    c2 = above_zero_complexity(y)\n",
    "    print(x, c1)\n",
    "    print(y, c2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_complexity('man','husband')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "5bc4e0d8c80a46683db44d5f27f3acefa8fbd5cfb58a17aa0a6dbfd83b51e145"
  },
  "kernelspec": {
   "display_name": "Python 3.10.4 ('reu')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
